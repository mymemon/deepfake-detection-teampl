from google.colab import drive
import os
import cv2
import random
import shutil
import zipfile
import logging
import time
import torch
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim

# Google Drive를 마운트하여 파일에 액세스합니다.
drive.mount('/content/drive')

# ------------------------------
# 1. 데이터 준비 설정
# ------------------------------
zip_file_path = '/content/drive/MyDrive/dfdc_train_part_00.zip'
data_folder = 'deepfake_dataset'
frames_folder = 'processed_frames'
train_folder = '/content/drive/MyDrive/deepfake_dataset/train'
val_folder = '/content/drive/MyDrive/deepfake_dataset/val'
frame_count = 5
split_ratio = 0.8
image_size = (64, 64)

# 로그 설정
logging.basicConfig(
    filename='program.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def log_and_print(message):
    logging.info(message)
    print(message)

def extract_frames_from_videos(data_folder, output_folder, frame_count, image_size):
    os.makedirs(output_folder, exist_ok=True)
    for class_name in os.listdir(data_folder):
        class_path = os.path.join(data_folder, class_name)
        output_class_folder = os.path.join(output_folder, class_name)
        os.makedirs(output_class_folder, exist_ok=True)

        start_time = time.time()
        log_and_print(f"Processing videos in class '{class_name}'...")
        video_count = 0

        for video_file in os.listdir(class_path):
            video_path = os.path.join(class_path, video_file)
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            interval = max(total_frames // (frame_count * 2), 1)

            frame_number = 0
            saved_frames = 0
            video_name = os.path.splitext(video_file)[0]

            while cap.isOpened() and saved_frames < frame_count:
                ret, frame = cap.read()
                if not ret:
                    break
                if frame_number % interval == 0:
                    frame_filename = os.path.join(output_class_folder, f"{video_name}_frame_{saved_frames}.jpg")
                    resized_frame = cv2.resize(frame, image_size)
                    cv2.imwrite(frame_filename, resized_frame)
                    saved_frames += 1
                frame_number += 1

            cap.release()
            video_count += 1

        end_time = time.time()
        log_and_print(f"Completed processing {video_count} videos in class '{class_name}' in {end_time - start_time:.2f} seconds.")

def split_dataset(input_folder, train_folder, val_folder, split_ratio):
    for class_name in os.listdir(input_folder):
        class_path = os.path.join(input_folder, class_name)
        train_class_folder = os.path.join(train_folder, class_name)
        val_class_folder = os.path.join(val_folder, class_name)

        os.makedirs(train_class_folder, exist_ok=True)
        os.makedirs(val_class_folder, exist_ok=True)

        files = os.listdir(class_path)
        random.shuffle(files)

        split_index = int(len(files) * split_ratio)
        train_files = files[:split_index]
        val_files = files[split_index:]

        start_time = time.time()
        log_and_print(f"Splitting dataset for class '{class_name}' into train and validation sets...")

        for file in train_files:
            shutil.copy(os.path.join(class_path, file), train_class_folder)

        for file in val_files:
            shutil.copy(os.path.join(class_path, file), val_class_folder)

        end_time = time.time()
        log_and_print(f"Completed splitting dataset for class '{class_name}' in {end_time - start_time:.2f} seconds.")

# ------------------------------
# 2. 데이터 준비 실행
# ------------------------------
log_and_print("Starting dataset preparation...")

# 압축 해제
log_and_print("Extracting dataset from zip file...")
start_time = time.time()
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(data_folder)
end_time = time.time()
log_and_print(f"Dataset extracted in {end_time - start_time:.2f} seconds.")

# 프레임 추출
log_and_print("Extracting frames from videos...")
extract_frames_from_videos(data_folder, frames_folder, frame_count, image_size)

# 데이터셋 분할
log_and_print("Splitting frames into train and validation sets...")
split_dataset(frames_folder, train_folder, val_folder, split_ratio)

# ------------------------------
# 3. 모델 학습 및 검증
# ------------------------------
# 학습 파라미터 설정
batch_size = 16
num_epochs = 10
learning_rate = 0.001

# GPU 사용 여부 체크
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 데이터 로드 및 확인
transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])
train_dataset = datasets.ImageFolder(train_folder, transform=transform)
val_dataset = datasets.ImageFolder(val_folder, transform=transform)

# DataLoader 설정
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

# ResNet18 모델 설정
model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))
model = model.to(device)

# 손실 함수 및 옵티마이저 설정
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# 학습 함수
def train_model(model, train_loader, criterion, optimizer, num_epochs, device):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}")

# 검증 함수
def validate_model(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()

    val_loss = running_loss / len(val_loader.dataset)
    accuracy = correct / len(val_loader.dataset)
    print(f"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}")

# 모델 학습 및 검증
train_model(model, train_loader, criterion, optimizer, num_epochs, device)
validate_model(model, val_loader, criterion, device)
